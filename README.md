# âœ¨ Air Writing with Gesture Recognition

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-orange.svg)](https://www.tensorflow.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-4.0+-green.svg)](https://opencv.org/)

> Transform your webcam into an interactive air writing canvas using hand gesture recognition! Write, erase, and create digital art with just your hand movements. ğŸ–ï¸âœï¸

<p align="center">
  <img src="/api/placeholder/800/400" alt="Air Writing Demo">
</p>

## ğŸŒŸ Features

- âœï¸ **Air Writing**: Draw in the air using hand gestures
- ğŸ¨ **Multiple Colors**: Choose from green, blue, red, and white
- ğŸ§¹ **Gesture-Based Erasing**: Erase content with a simple hand gesture
- ğŸ¯ **Real-time Recognition**: Instant response to hand movements
- ğŸšï¸ **Adjustable Controls**: 
  - Drawing thickness
  - Canvas opacity
  - Canvas color (black/white)
- ğŸ’ª **Gesture Support**:
  - "Write" - Activate drawing mode
  - "Move" - Navigate without drawing
  - "Erase" - Clear selected area

## ğŸš€ Getting Started

### Prerequisites

```bash
python -m pip install -r requirements.txt
```

Required packages:
- tensorflow
- opencv-python
- mediapipe
- numpy
- pandas
- scikit-learn
- joblib

### ğŸ¯ Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/satyam-singhxx/AIG-Smart-Board.git
cd AGI-smart-board
```

2. **Train the model** (optional - pre-trained model included)
```bash
python model_building.py
```

3. **Run the application**
```bash
python app.py
```

## ğŸ’¡ Usage

1. **Data Collection**
```bash
python data_collection.py
```
- Follow on-screen instructions to record hand gestures
- Performs automatic data labeling and saving

2. **Model Training**
```bash
python model_1.py
```
- Trains the LSTM model on collected data
- Generates performance visualizations
- Saves the trained model

3. **Running the App**
```bash
python app_v2_h5.py
```

### ğŸ® Controls

- **Gesture Controls**:
  - Write: Make a writing gesture
  - Move: Open palm
  - Erase: Closed fist

- **Keyboard Controls**:
  - `w`: Switch to white canvas
  - `b`: Switch to black canvas
  - `c`: Clear canvas
  - `q`: Quit application

- **UI Controls**:
  - Top color palette: Select drawing color
  - Bottom sliders: Adjust thickness and opacity

## ğŸ“Š Model Architecture

```
Model: Sequential
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LSTM               â”‚ (None, 64) â”‚
â”‚ Dropout (0.2)      â”‚           â”‚
â”‚ LSTM               â”‚ (None, 32) â”‚
â”‚ Dropout (0.2)      â”‚           â”‚
â”‚ Dense (softmax)    â”‚ (None, 3)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
air-writing-recognition/
â”œâ”€â”€ data_collection.py      # Data collection script
â”œâ”€â”€ model_building.py       # Model training script
â”œâ”€â”€ app.py                 # Main application
â”œâ”€â”€ data/                  # Collected data
â”‚   â”œâ”€â”€ hand_keypoints.csv
â”‚   â”œâ”€â”€ hand_historical.csv
â”‚   â””â”€â”€ gesture_enum.csv
â”œâ”€â”€ models/                # Trained models
â”‚   â”œâ”€â”€ hand_gesture_model.h5
â”‚   â””â”€â”€ scaler.pkl
â””â”€â”€ visualizations/        # Training visualizations
    â”œâ”€â”€ training_history.png
    â”œâ”€â”€ confusion_matrix.png
    â””â”€â”€ class_metrics.png
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- MediaPipe for hand tracking
- TensorFlow for deep learning capabilities
- OpenCV for image processing
- All contributors and supporters of the project

---

<p align="center">
  Made with â¤ï¸ by [Your Name]
</p>
